{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implements algorithm 3 (ModMod) from Algorithms for \"Approx Min of the Difference Between Submodular Fncs with Applications.\"\n",
    "\n",
    "For prob_ssm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import distributions as dist\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "import operator\n",
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper')\n",
    "# Plot adjustments:\n",
    "plt.rcParams.update({'ytick.labelsize': 15})\n",
    "plt.rcParams.update({'xtick.labelsize': 15})\n",
    "plt.rcParams.update({'axes.labelsize': 35})\n",
    "plt.rcParams.update({'legend.fontsize': 30})\n",
    "plt.rcParams.update({'axes.titlesize': 16})\n",
    "\n",
    "from gptorch import kernels, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mod_lower(X, fn, perm, *args, **kwargs):\n",
    "    \"\"\" Modular lower bound of fn(X) for any X contained in ground set V\n",
    "    with permutation chain perm (aka S).\n",
    "    \n",
    "    Expects X as a list of tuples, fn as a Python function, and perm as a list. \"\"\"\n",
    "    \n",
    "    low = 0.0 # lower modular bound\n",
    "    \n",
    "    for elem in X:\n",
    "        i = perm.index(elem)\n",
    "        if i == 0:\n",
    "            low += fn([perm[0]], *args, **kwargs)\n",
    "        else:\n",
    "            low += fn(perm[0:i + 1], *args, **kwargs) - fn(perm[0:i], *args, **kwargs)\n",
    "            \n",
    "    return low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mod_upper(X, fn, center, *args, **kwargs):\n",
    "    \"\"\" Modular upper bound of fn(X) for any X contained in ground set V, \n",
    "    centered at center.\n",
    "    \n",
    "    Expects X and center as lists of tuples, and fn as a Python function. \"\"\"\n",
    "    \n",
    "    up = fn(center, *args, **kwargs) # modular upper bound\n",
    "\n",
    "    for j in center:\n",
    "        if j not in X:\n",
    "            center_noj = [x for x in center if x != j]\n",
    "            up -= fn(center, *args, **kwargs) - fn(center_noj, *args, **kwargs)\n",
    "\n",
    "#     for j in X:\n",
    "#         if j not in center:\n",
    "#             up += fn([j], *args, **kwargs) - fn([], *args, **kwargs)\n",
    "            \n",
    "    return up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_perm(V, X):\n",
    "    \"\"\" Takes in the ground set V and a set X, and\n",
    "    returns a random chain permutation that contains X \"\"\"\n",
    "    if len(X) == 0:\n",
    "        indices = list(range(len(V)))\n",
    "        random.shuffle(indices)\n",
    "        return [V[i] for i in indices]\n",
    "    \n",
    "    ind_X = [i for i, v in enumerate(V) if v in X] # indices of X in V\n",
    "    rest = [i for i in list(range(len(V))) if i not in ind_X] # rest of indices in V\n",
    "    \n",
    "    random.shuffle(ind_X) # shuffle indices\n",
    "    random.shuffle(rest)\n",
    "    indices = ind_X + rest # combine\n",
    "    \n",
    "    return [V[i] for i in indices] # generate perm based on shuffled indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mod_mod(V, fn, g, seed):\n",
    "    \"\"\" Implements algorithm3 (ModMod) from paper. Takes in ground set V,\n",
    "    and functions fn and g. \n",
    "    \n",
    "    Expects V as a list of tuples, and fn and g as submodular Python functions. \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    it = 0 # keeps track of iterations in while loop\n",
    "    \n",
    "    while True:\n",
    "        it += 1\n",
    "        X_next = []\n",
    "        perm = make_perm2(V, X) # choose permutation\n",
    "        \n",
    "        if it == 1:\n",
    "            emp = mod_upper(seed, fn, X) - mod_lower(seed, g, perm) # obj w seed as input\n",
    "            for i in V:\n",
    "                obj = mod_upper([i], fn, X) - mod_lower([i], g, perm) # obj w element in V as input\n",
    "                if obj < emp:\n",
    "                    X_next.append(i)\n",
    "        else:\n",
    "            emp = mod_upper(X, fn, X) - mod_lower(X, g, perm) # obj w X as input\n",
    "            for i in V:\n",
    "                if i in X: \n",
    "                    X_noi = [x for x in X if x != i]\n",
    "                    obj = mod_upper(X_noi, fn, X) - mod_lower(X_noi, g, perm) # obj w X w/o element in V as input\n",
    "                else:\n",
    "                    obj = mod_upper(X + [i], fn, X) - mod_lower(X + [i], g, perm) # obj w element in V added to X as input\n",
    "                if obj < emp:\n",
    "                    X_next.append(i)\n",
    "        if X_next == X:\n",
    "            break\n",
    "        else:\n",
    "            X = X_next\n",
    "    \n",
    "    return X_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# [['A', 'R'], ['V'], ['P'], ['F', 'Q']] - turn into list of lists first!\n",
    "\n",
    "# lst = [('F', 3), ('V', 1), ('P', 2), ('A', 0), ('R', 0), ('Q', 3), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../inputs/phoq.pkl', 'rb') as f:\n",
    "    t = pickle.load(f)\n",
    "\n",
    "X = t[0] # one-hot encoding of X\n",
    "T = t[1] # tokenized encoding of X\n",
    "y = t[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_X(X):\n",
    "    \"\"\" Takes in one-hot encoding X and decodes it to\n",
    "    return a string of four amino acids. \"\"\"\n",
    "    \n",
    "    amino_acids = 'ARNDCQEGHILKMFPSTWYV'\n",
    "    \n",
    "    pos_X = [i for i, x in enumerate(X) if x == 1.0] # positions of amino acids\n",
    "    pos_X = [(p - 20 * i) for i, p in enumerate(pos_X)] # make sure indexing is same as in str amino_acids\n",
    "    aa_X = [amino_acids[p] for i, p in enumerate(pos_X)] # amino acid chars in X\n",
    "    return ''.join(aa_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Iteration 1 of 10\tNLML: 45.3735\t\r",
      "Iteration 2 of 10\tNLML: 44.1491\t\r",
      "Iteration 3 of 10\tNLML: 43.1219\t\r",
      "Iteration 4 of 10\tNLML: 42.1918\t\r",
      "Iteration 5 of 10\tNLML: 41.3986\t\r",
      "Iteration 6 of 10\tNLML: 40.8094\t\r",
      "Iteration 7 of 10\tNLML: 40.4363\t\r",
      "Iteration 8 of 10\tNLML: 40.2570\t\r",
      "Iteration 9 of 10\tNLML: 40.2296\t\r",
      "Iteration 10 of 10\tNLML: 40.2686\t"
     ]
    }
   ],
   "source": [
    "def get_predictions(X_train, y_train, X_test, its=500):\n",
    "    \"\"\"\n",
    "    Train GP regressor on X_train and y_train. \n",
    "    Predict mean and std for X_test. \n",
    "    Return P(y > y_train_max) as dictionary eg 'AGHU': 0.78\n",
    "    NB: for X_test in X_train, P ~= 0\n",
    "    Be careful with normalization\n",
    "    \n",
    "    Expects X_train, y_train, and X_test as np.arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    ke = kernels.MaternKernel()\n",
    "    mo = models.GPRegressor(ke)\n",
    "    \n",
    "    # make data into tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    X_test = torch.Tensor(np.array(X_test))\n",
    "    y_train_scaled = (np.array(y_train) - np.mean(np.array(y_train))) / np.std(np.array(y_train)) # scale y_train\n",
    "    y_train_scaled = torch.Tensor(y_train_scaled.reshape(len(y_train_scaled), 1)) # .float()\n",
    "    \n",
    "    his = mo.fit(X_train, y_train_scaled, its=its) # fit model with training set\n",
    "    \n",
    "    # make predictions\n",
    "    dic = {} # use dictionary to store probs\n",
    "    ind = 0 # index for feeding in batches of X_test\n",
    "    tau = y_train_scaled.max().float()\n",
    "    \n",
    "    for i in range(1000, len(X) + 1000, 1000):\n",
    "        mu, var = mo.forward(X_test[ind:i]) # make predictions\n",
    "        std = torch.sqrt(var.diag())\n",
    "        mu = mu.squeeze()\n",
    "        prob = 1 - dist.Normal(mu, std).cdf(tau) # compute probabilities for all means, stds\n",
    "\n",
    "        for j, p in enumerate(prob):\n",
    "            seq = decode_X(X_test[ind:i][j]) # decode one-hot to get string of seq\n",
    "            dic[seq] = p # store prob for each seq\n",
    "\n",
    "        ind = i\n",
    "        \n",
    "    return dic\n",
    "\n",
    "np.random.seed(1)\n",
    "rand_inds = np.random.choice(len(X), 100, replace=True) # generate random indices for 100 X's to sample from\n",
    "X_train = X[rand_inds]\n",
    "y_train = y[rand_inds]\n",
    "X_test = X\n",
    "y_true = y\n",
    "\n",
    "dic = get_predictions(X_train, y_train, X_test, its=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing objective LHS and RHS (two supermodular set functions)\n",
    "\n",
    "def obj_LHS(X, probs):\n",
    "    \"\"\" Takes in library X, and probabilities.\n",
    "    \n",
    "    Expects X to be a list of tuples, and probs to be a dictionary.\n",
    "    \n",
    "    Returns LHS of objective to be maximized (a supermodular function):\n",
    "    sum of probabilities. \"\"\"\n",
    "    \n",
    "    # if X empty or does not have aa at each position, return 0\n",
    "    if len([i for i in range(4) if i not in [tup[1] for tup in X]]) > 0:\n",
    "        return torch.Tensor([0.0])[0]\n",
    "    \n",
    "    # filter thru probs to find prob of x's in X\n",
    "    X.sort(key=lambda tup: tup[1])\n",
    "\n",
    "    X_str = [[tup[0] for i, tup in enumerate(X) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "    X_str = [''.join(s) for s in itertools.product(*X_str)] # generate list of strings of 4 aa seqs\n",
    "\n",
    "    p = torch.Tensor([probs[key] for key in X_str])\n",
    "    \n",
    "    return -1 * torch.sum(p)\n",
    "\n",
    "def obj_RHS(X, probs, n):\n",
    "    \"\"\" Takes in library X, probabilities, and batch size n.\n",
    "    \n",
    "    Expects X to be a list of tuples, and probs to be a dictionary.\n",
    "    \n",
    "    Returns RHS of objective to be maximized (a supermodular function):\n",
    "    sum of probabilities times expression with N and n. \"\"\"\n",
    "    \n",
    "    # if X empty or does not have aa at each position, return 0\n",
    "    if len([i for i in range(4) if i not in [tup[1] for tup in X]]) > 0:\n",
    "        return torch.Tensor([0.0])[0]\n",
    "    \n",
    "    N = 1 # represents the product of sequence of # aas at each position\n",
    "    for i in X:\n",
    "        N *= len(i)\n",
    "    \n",
    "    # filter thru probs to find prob of x's in X\n",
    "    X.sort(key=lambda tup: tup[1])\n",
    "\n",
    "    X_str = [[tup[0] for i, tup in enumerate(X) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "    X_str = [''.join(s) for s in itertools.product(*X_str)] # generate list of strings of 4 aa seqs\n",
    "\n",
    "    p = torch.Tensor([probs[key] for key in X_str])\n",
    "    obj = torch.sum(p) * (1 - 1 / N) ** n\n",
    "    \n",
    "    return -1 * obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_V():\n",
    "    \"\"\" Returns V: a list of tuples with every possible amino acid\n",
    "    at each of the four positions. \"\"\"\n",
    "    \n",
    "    amino_acids = 'ARNDCQEGHILKMFPSTWYV'\n",
    "    return [(aa, i) for i in range(4) for aa in amino_acids]\n",
    "\n",
    "V = generate_V()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = [('F', 3), ('V', 1), ('P', 2), ('A', 0), ('R', 0), ('Q', 3)]\n",
    "center = [('F', 3), ('E', 0), ('N', 1), ('G', 2)]#, ('M', 3), ('S', 2), ('W', 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seed(probs):\n",
    "    \"\"\" Takes in a dictionary of amino acids to probabilities as\n",
    "    generated by the get_predictions() function, and returns the \n",
    "    seed (the four amino acid seq with the best prediction, aka the \n",
    "    highest probabilitiy). \n",
    "    \n",
    "    Returns a list of tuples representing the seed.\n",
    "    \n",
    "    Currently, 'SSSG' is the seed. \"\"\"\n",
    "    \n",
    "    seq = max(probs.items(), key=operator.itemgetter(1))[0]\n",
    "    return [(aa, i) for aa, i in zip(seq, range(4))]\n",
    "\n",
    "seed = get_seed(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "perm = make_perm(V, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-04 *\n",
       "       -1.8430)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_LHS(X, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-04 *\n",
       "       3.5167)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_upper(X, obj_LHS, center, dic) ## should match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-04 *\n",
       "       3.5167)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = torch.stack([mod_upper([x], obj_LHS, center, dic) for x in X]).sum()\n",
    "A -= (len(X) - 1) * mod_upper([], obj_LHS, center, dic) # takes into account empty set ## should match\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.00000e-04 *\n",
       "       -1.8430)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod_lower(X, obj_LHS, perm, dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.00000e-04 *\n",
      "       -1.8430)\n"
     ]
    }
   ],
   "source": [
    "print(torch.stack([mod_lower([x], obj_LHS, perm, dic) for x in X]).sum()) # lower bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "obj_RHS(X, dic, 100, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_lower(X, obj_RHS, perm, dic, 100, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mod_upper(X, obj_RHS, center, dic, 100, seed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
