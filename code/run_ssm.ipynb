{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, makes predictions on a library of 160,000 PhoQ variants using GP and Matern Kernel then computes objective. Combines gp_ssm and objective_ssm notebooks. \n",
    "\n",
    "Includes functions that compute each of the three baselines:\n",
    "1. Baseline that creates optimal sequence from X's given optimal amino acids (those with max y-values) at each position out of the four possible positions in the wildtype sequence by fixing the three other positions, then continues onto the next position by fixing the best amino acid in the previous position.\n",
    "2. Baseline that creates optimal sequence from X's given optimal amino acids (those with max y-values) at each position out of the four possible positions in the wildtype sequence by fixing the three other positions, then takes the best amino acid at each position.\n",
    "3. Baseline that uses greedy algorithm to maximize objective. Starts out with best prediction then continues to add amino acids until objective stops increasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import distributions as dist\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from scipy.stats import norm\n",
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "sns.set_style('white')\n",
    "sns.set_context('paper')\n",
    "# Plot adjustments:\n",
    "plt.rcParams.update({'ytick.labelsize': 15})\n",
    "plt.rcParams.update({'xtick.labelsize': 15})\n",
    "plt.rcParams.update({'axes.labelsize': 35})\n",
    "plt.rcParams.update({'legend.fontsize': 30})\n",
    "plt.rcParams.update({'axes.titlesize': 16})\n",
    "\n",
    "from gptorch import kernels, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../inputs/phoq.pkl', 'rb') as f:\n",
    "    t = pickle.load(f)\n",
    "\n",
    "X = t[0] # one-hot encoding of X\n",
    "T = t[1] # tokenized encoding of X\n",
    "y = t[2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_X(X):\n",
    "    \"\"\" Takes in one-hot encoding X and decodes it to\n",
    "    return a string of four amino acids. \"\"\"\n",
    "    \n",
    "    amino_acids = 'ARNDCQEGHILKMFPSTWYV'\n",
    "    \n",
    "    pos_X = [i for i, x in enumerate(X) if x == 1.0] # positions of amino acids\n",
    "    pos_X = [(p - 20 * i) for i, p in enumerate(pos_X)] # make sure indexing is same as in str amino_acids\n",
    "    aa_X = [amino_acids[p] for i, p in enumerate(pos_X)] # amino acid chars in X\n",
    "    return ''.join(aa_X)\n",
    "\n",
    "# test on AWSS\n",
    "# decode_X([ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
    "#         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#         0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#         0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
    "#         0.,  0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## UPDATED VERSION OF GP_train() method\n",
    "\n",
    "def get_predictions(X_train, y_train, X_test, its=500, *args, **kwargs):\n",
    "    \"\"\"\n",
    "    Train GP regressor on X_train and y_train. \n",
    "    Predict mean and std for X_test. \n",
    "    Return P(y > y_train_max) as dictionary eg 'AGHU': 0.78\n",
    "    NB: for X_test in X_train, P ~= 0\n",
    "    Be careful with normalization\n",
    "    \n",
    "    Expects X_train, y_train, and X_test as np.arrays\n",
    "    \"\"\"\n",
    "    \n",
    "    ke = kernels.MaternKernel()\n",
    "    mo = models.GPRegressor(ke)\n",
    "    \n",
    "    # make data into tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    X_test = torch.Tensor(np.array(X_test))\n",
    "    y_train_scaled = (np.array(y_train) - np.mean(np.array(y_train))) / np.std(np.array(y_train)) # scale y_train\n",
    "    y_train_scaled = torch.Tensor(y_train_scaled.reshape(len(y_train_scaled), 1))\n",
    "    \n",
    "    his = mo.fit(X_train, y_train_scaled, its=its, *args, **kwargs) # fit model with training set\n",
    "    \n",
    "    # make predictions\n",
    "    dic = {} # use dictionary to store probs\n",
    "    ind = 0 # index for feeding in batches of X_test\n",
    "    tau = y_train_scaled.max().float()\n",
    "    \n",
    "    means = []\n",
    "    for i in range(1000, len(X) + 1000, 1000):\n",
    "        mu, var = mo.forward(X_test[ind:i]) # make predictions\n",
    "        std = torch.sqrt(var.diag())\n",
    "        mu = mu.squeeze()\n",
    "        prob = 1 - dist.Normal(mu, std).cdf(tau) # compute probabilities for all means, stds\n",
    "\n",
    "        for j, p in enumerate(prob):\n",
    "            seq = decode_X(X_test[ind:i][j])\n",
    "            dic[seq] = p # store prob for each seq\n",
    "            means.append(mu[j])\n",
    "\n",
    "        ind = i\n",
    "        \n",
    "    return dic, means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "rand_inds = np.random.choice(len(X), 100, replace=True) # generate random indices for 100 X's to sample from\n",
    "X_train = X[rand_inds]\n",
    "y_train = y[rand_inds]\n",
    "X_test = X\n",
    "y_true = y\n",
    "\n",
    "dic, means = get_predictions(X_train, y_train, X_test, its=500)\n",
    "\n",
    "preds = [] # to keep track of predictions after each iteration through greedy algorithm\n",
    "preds.append(means)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Computing objective\n",
    "\n",
    "# def objective1(X, probs, n):\n",
    "#     \"\"\" Takes in library X, probabilities, and batch size n.\n",
    "    \n",
    "#     Expects X to be a list of tuples, and probs to be a dictionary.\n",
    "    \n",
    "#     Returns objective to be maximized. \"\"\"\n",
    "    \n",
    "#     N = 1 # represents the product of sequence of # aas at each position\n",
    "#     for i in X:\n",
    "#         N *= len(i)\n",
    "    \n",
    "#     # filter thru probs to find prob of x's in X\n",
    "#     X.sort(key=lambda tup: tup[1])\n",
    "\n",
    "#     X_str = [[tup[0] for i, tup in enumerate(X) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "#     X_str = [''.join(s) for s in itertools.product(*X_str)] # generate list of strings of 4 aa seqs\n",
    "\n",
    "#     p = torch.Tensor([probs[key] for key in X_str])\n",
    "#     obj = torch.sum(p) * (1 - (1 - 1 / N) ** n)\n",
    "    \n",
    "#     return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Computing objective LHS and RHS (two supermodular set functions) - objective to be minimized\n",
    "\n",
    "def obj_LHS(X, probs):\n",
    "    \"\"\" Takes in library X, and probabilities.\n",
    "    \n",
    "    Expects X to be a list of tuples, and probs to be a dictionary.\n",
    "    \n",
    "    Returns LHS of objective to be maximized (a supermodular function):\n",
    "    sum of probabilities. \"\"\"\n",
    "    \n",
    "    # if X empty or does not have aa at each position, return 0\n",
    "    if len([i for i in range(4) if i not in [tup[1] for tup in X]]) > 0:\n",
    "        return torch.Tensor([0.0])[0]\n",
    "    \n",
    "    # filter thru probs to find prob of x's in X\n",
    "    X.sort(key=lambda tup: tup[1])\n",
    "\n",
    "    X_str = [[tup[0] for i, tup in enumerate(X) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "    X_str = [''.join(s) for s in itertools.product(*X_str)] # generate list of strings of 4 aa seqs\n",
    "\n",
    "    p = torch.Tensor([probs[key] for key in X_str])\n",
    "    \n",
    "    return -1 * torch.sum(p)\n",
    "\n",
    "def obj_RHS(X, probs, n):\n",
    "    \"\"\" Takes in library X, probabilities, and batch size n.\n",
    "    \n",
    "    Expects X to be a list of tuples, and probs to be a dictionary.\n",
    "    \n",
    "    Returns RHS of objective to be maximized (a supermodular function):\n",
    "    sum of probabilities times expression with N and n. \"\"\"\n",
    "    \n",
    "    # if X empty or does not have aa at each position, return 0\n",
    "    if len([i for i in range(4) if i not in [tup[1] for tup in X]]) > 0:\n",
    "        return torch.Tensor([0.0])[0]\n",
    "    \n",
    "    N = 1 # represents the product of sequence of # aas at each position\n",
    "    for i in X:\n",
    "        N *= len(i)\n",
    "    \n",
    "    # filter thru probs to find prob of x's in X\n",
    "    X.sort(key=lambda tup: tup[1])\n",
    "\n",
    "    X_str = [[tup[0] for i, tup in enumerate(X) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "    X_str = [''.join(s) for s in itertools.product(*X_str)] # generate list of strings of 4 aa seqs\n",
    "\n",
    "    p = torch.Tensor([probs[key] for key in X_str])\n",
    "    obj = torch.sum(p) * (1 - 1 / N) ** n\n",
    "    \n",
    "    return -1 * obj\n",
    "\n",
    "def objective(X, probs, n):\n",
    "    \"\"\" Objective (negative of objective to be maximized) to be minimized. \"\"\"\n",
    "    return obj_LHS(X, probs) - obj_RHS(X, probs, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES/RETURNS STRINGS, RETURNS ALL 24 POSSIBILITIES\n",
    "\n",
    "def baseline_fixed(wt, X, y): # deterministic\n",
    "    \"\"\" Takes in wildtype sequence, X, and y to compute baseline that creates \n",
    "    optimal sequence from X's given optimal amino acids (those with max y-values) \n",
    "    at each position out of the four possible positions in the wildtype sequence \n",
    "    by fixing the three other positions, then continues onto the next position in\n",
    "    the wildtype sequence by fixing the best amino acid in the previous position.\n",
    "    So the fixed substring is not necessarily a fixed substring of the wildtype sequence.\n",
    "    \n",
    "    Note: wildtype sequence expected as string. X expected as an array or list of\n",
    "    one-hot encodings.\n",
    "    \n",
    "    Returns list of all possible 24 optimal untested variants (as a string). \"\"\"  \n",
    "    \n",
    "    X_decode = [decode_X(x) for x in X]\n",
    "    baseline = []\n",
    "    \n",
    "    perms = list(itertools.permutations(np.arange(len(wt)))) # list of all possible permutations for picking which aa to vary\n",
    "    \n",
    "    for perm in perms:\n",
    "        seq = list(wt) # seq starts out as wt seq, will store variant after iteration through perm\n",
    "        \n",
    "        for i in perm:\n",
    "            fixed = ''.join(seq) # fixed substring\n",
    "\n",
    "            # index of xs in X with fixed substring\n",
    "            index = [j for j, x in enumerate(X_decode) if fixed[0:i] == x[0:i] and fixed[i + 1:len(fixed)] == x[i + 1:len(x)]] \n",
    "            ys = [y[j] for j in index] # stores y values of x's in X with those 3 fixed amino acids\n",
    "\n",
    "            max_ind = np.where(ys==max(ys))[0][0] # takes first occurrence of index with maximum y value\n",
    "\n",
    "            seq[i] = X_decode[index[max_ind]][i]\n",
    "  \n",
    "        baseline.append(''.join(seq))\n",
    "    \n",
    "    return baseline\n",
    "\n",
    "wt = decode_X(X[150614]) # wt as string\n",
    "seqs = baseline_fixed(wt, X, y)\n",
    "print(\"aa: {}\".format(seqs))\n",
    "\n",
    "# find y-values corresponding to 24 possible baselines from baseline_fixed() --> take aa seq with max y\n",
    "\n",
    "seqs = list(set(seqs)) # remove duplicates\n",
    "X_decode = [decode_X(x) for x in X]\n",
    "ys_baseline = [y[X_decode.index(x)] for x in seqs]\n",
    "max_baseline = seqs[ys_baseline.index(max(ys_baseline))]\n",
    "\n",
    "y_seq1 = max(ys_baseline)\n",
    "print(\"best baseline: {}\".format(max_baseline))\n",
    "print(\"y value: {}\".format(y_seq1))\n",
    "print(\"global max: {}\".format(np.max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USES/RETURNS STRINGS\n",
    "\n",
    "def baseline_vary(wt, X, y): # deterministic\n",
    "    \"\"\" Takes in wildtype sequence, X, and y to compute baseline that creates \n",
    "    optimal sequence from X's given optimal amino acids (those with max y-values) at each \n",
    "    position out of the four possible positions in the wildtype sequence by fixing the three\n",
    "    other positions, then takes the best amino acid at each position. The fixed substring\n",
    "    in each iteration is a substring of the wildtype sequence.\n",
    "    \n",
    "    Note: wildtype sequence expected as a string. X expected as an array or list of\n",
    "    one-hot encodings.\n",
    "    \n",
    "    Returns optimal untested variant (as a string). \"\"\"\n",
    "    \n",
    "    X_decode = [decode_X(x) for x in X]\n",
    "    baseline = \"\" # stores baseline untested variant to be returned\n",
    "    wt = list(wt)\n",
    "    \n",
    "    for i in range(4): # vary amino acid in each position\n",
    "        fixed = ''.join(wt) # list of 3 fixed amino acids in each iteration through wt seq\n",
    "        \n",
    "        # index of xs in X with fixed substring\n",
    "        index = [j for j, x in enumerate(X_decode) if fixed[0:i] == x[0:i] and fixed[i + 1:len(fixed)] == x[i + 1:len(x)]] \n",
    "        ys = [y[j] for j in index] # stores y values of x's in X with those 3 fixed amino acids\n",
    "        \n",
    "        max_ind = np.where(ys==max(ys))[0][0] # takes first occurrence of index with maximum y value\n",
    "\n",
    "        # store amino acid in position being varied in baseline\n",
    "        baseline += X_decode[index[max_ind]][i]\n",
    "    \n",
    "    return baseline\n",
    "\n",
    "wt = decode_X(X[150614])  # wt as string\n",
    "seq2 = baseline_vary(wt, X, y)\n",
    "print(\"aa: {}\".format(seq2))\n",
    "\n",
    "y_seq2 = y[X_decode.index(seq2)]\n",
    "print(\"y value: {}\".format(y_seq2))\n",
    "print(\"global max: {}\".format(np.max(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def avail_aa(X):\n",
    "    \"\"\" Takes in a library X and returns a list of tuples of the available amino acids \n",
    "    at each position that can be added to the library. \"\"\"\n",
    "    \n",
    "    amino_acids = 'ARNDCQEGHILKMFPSTWYV'\n",
    "    return [(aa, i) for i in range(4) for aa in amino_acids if (aa, i) not in X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_greedy(probs, seed, n):\n",
    "    \"\"\" Takes in probabilities, seed (the best prediction), and batch size\n",
    "    to create baseline optimal library using the Greedy algorithm. The algo \n",
    "    starts out with the seed then continues to add amino acids until obj\n",
    "    stops increasing.\n",
    "    \n",
    "    Note: probs expected as a dictionary, and seed expected as list of tuples.\n",
    "    \n",
    "    Returns optimal untested library. \"\"\"\n",
    "    \n",
    "    X = seed # library X starts with seed\n",
    "    \n",
    "    obj = objective(X, probs, n)\n",
    "    aa = avail_aa(X) # determine available/unincluded amino acids at each position of X\n",
    "\n",
    "    while True:\n",
    "        lst = [objective(X + [a], probs, n) for a in aa] # lst of obj's for library w each available aa added\n",
    "\n",
    "        index, obj_next = min(enumerate(lst), key=operator.itemgetter(1)) # determine which aa maximizes obj\n",
    "        if obj_next > obj: # if obj stops decreasing, exit\n",
    "            break\n",
    "        else:\n",
    "            X.append(aa[index]) # add aa that maximizes obj to X\n",
    "            obj = obj_next\n",
    "            aa.remove(aa[index])\n",
    "            \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_seed(probs):\n",
    "    \"\"\" Takes in a dictionary of amino acids to probabilities as\n",
    "    generated by the get_predictions() function, and returns the \n",
    "    seed (the four amino acid seq with the best prediction, aka the \n",
    "    highest probabilitiy). \n",
    "    \n",
    "    Returns a list of tuples representing the seed.\n",
    "    \n",
    "    Currently, 'SSSG' is the seed. \"\"\"\n",
    "    \n",
    "    seq = max(probs.items(), key=operator.itemgetter(1))[0]\n",
    "    return [(aa, i) for aa, i in zip(seq, range(4))]\n",
    "\n",
    "seed = get_seed(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_X(X):\n",
    "    \"\"\" Takes in a string of four amino acids and encodes it\n",
    "    to return a one-hot encoding. \"\"\"\n",
    "    \n",
    "    amino_acids = 'ARNDCQEGHILKMFPSTWYV'\n",
    "    \n",
    "    enc = np.array([0.] * 80)\n",
    "    pos_X = [amino_acids.find(char) for char in X] # positions of amino acids\n",
    "    for i, pos in enumerate(pos_X):\n",
    "        enc[pos + i * 20] = 1.0\n",
    "    return enc\n",
    "\n",
    "# test on AWSS\n",
    "# [ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#   0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#   0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
    "#   0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#   0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
    "#   0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
    "#   0.,  0.])\n",
    "\n",
    "# encode_X(\"AWSS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ##it should eventually come to a place where adding \n",
    "# #items to the set doesn't improve the objective (or it's added everything to the set)\n",
    "\n",
    "# lib_prev = []\n",
    "# lst_ys = []\n",
    "# it = 0\n",
    "# while True:\n",
    "#     it += 1\n",
    "#     print(it)\n",
    "#     lib = baseline_greedy(dic, seed, 100)\n",
    "#     lib.sort(key=lambda tup: tup[1])\n",
    "\n",
    "#     lib_str = [[tup[0] for i, tup in enumerate(lib) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "#     lib_str = [''.join(s) for s in itertools.product(*lib_str)] # generate list of strings of 4 aa seqs\n",
    "#     encs = np.array([encode_X(x) for x in lib_str]) # generate np array of one-hot encodings\n",
    "    \n",
    "#     X_decode = [decode_X(x) for x in X]\n",
    "    \n",
    "#     index = [X_decode.index(x) for x in lib_str]\n",
    "#     y_new = np.array([y[i] for i in index])\n",
    "#     lst_ys.append(y_new)\n",
    "    \n",
    "#     dic, means = get_predictions(encs, y_new, X, its=500)\n",
    "#     preds.append(means)\n",
    "#     seed = get_seed(dic)\n",
    "#     lib = baseline_greedy(dic, seed, 100)\n",
    "#     print(lib)\n",
    "    \n",
    "#     if lib == lib_prev:\n",
    "#         break\n",
    "#     if it == 5:\n",
    "#         break\n",
    "        \n",
    "#     lib_prev = lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##it should eventually come to a place where adding \n",
    "#items to the set doesn't improve the objective (or it's added everything to the set)\n",
    "\n",
    "\n",
    "## FIRST ITERATION\n",
    "lst_ys = []\n",
    "libs = []\n",
    "\n",
    "lib = baseline_greedy(dic, seed, 100)\n",
    "print(lib)\n",
    "lib.sort(key=lambda tup: tup[1])\n",
    "\n",
    "lib_str = [[tup[0] for i, tup in enumerate(lib) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "lib_str = [''.join(s) for s in itertools.product(*lib_str)] # generate list of strings of 4 aa seqs\n",
    "libs.append(lib_str)\n",
    "encs = np.array([encode_X(x) for x in lib_str]) # generate np array of one-hot encodings\n",
    "\n",
    "X_decode = [decode_X(x) for x in X]\n",
    "\n",
    "index = [X_decode.index(x) for x in lib_str]\n",
    "y_new = np.array([y[i] for i in index])\n",
    "lst_ys.append(y_new)\n",
    "\n",
    "dic, means = get_predictions(encs, y_new, X, its=500)\n",
    "preds.append(means)\n",
    "seed = get_seed(dic)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SECOND ITERATION\n",
    "lib = baseline_greedy(dic, seed, 100)\n",
    "print(lib)\n",
    "lib.sort(key=lambda tup: tup[1])\n",
    "\n",
    "lib_str = [[tup[0] for i, tup in enumerate(lib) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "lib_str = [''.join(s) for s in itertools.product(*lib_str)] # generate list of strings of 4 aa seqs\n",
    "libs.append(lib_str)\n",
    "encs = np.array([encode_X(x) for x in lib_str]) # generate np array of one-hot encodings\n",
    "\n",
    "index = [X_decode.index(x) for x in lib_str]\n",
    "y_new = np.array([y[i] for i in index])\n",
    "lst_ys.append(y_new)\n",
    "\n",
    "dic, means = get_predictions(encs, y_new, X, its=500)\n",
    "preds.append(means)\n",
    "seed = get_seed(dic)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## THIRD ITERATION\n",
    "lib = baseline_greedy(dic, seed, 100)\n",
    "print(lib)\n",
    "lib.sort(key=lambda tup: tup[1])\n",
    "\n",
    "lib_str = [[tup[0] for i, tup in enumerate(lib) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "lib_str = [''.join(s) for s in itertools.product(*lib_str)] # generate list of strings of 4 aa seqs\n",
    "libs.append(lib_str)\n",
    "encs = np.array([encode_X(x) for x in lib_str]) # generate np array of one-hot encodings\n",
    "\n",
    "index = [X_decode.index(x) for x in lib_str]\n",
    "y_new = np.array([y[i] for i in index])\n",
    "lst_ys.append(y_new)\n",
    "\n",
    "dic, means = get_predictions(encs, y_new, X, its=500)\n",
    "preds.append(means)\n",
    "seed = get_seed(dic)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FOURTH ITERATION\n",
    "lib = baseline_greedy(dic, seed, 100)\n",
    "print(lib)\n",
    "lib.sort(key=lambda tup: tup[1])\n",
    "\n",
    "lib_str = [[tup[0] for i, tup in enumerate(lib) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "lib_str = [''.join(s) for s in itertools.product(*lib_str)] # generate list of strings of 4 aa seqs\n",
    "libs.append(lib_str)\n",
    "encs = np.array([encode_X(x) for x in lib_str]) # generate np array of one-hot encodings\n",
    "\n",
    "index = [X_decode.index(x) for x in lib_str]\n",
    "y_new = np.array([y[i] for i in index])\n",
    "lst_ys.append(y_new)\n",
    "\n",
    "dic, means = get_predictions(encs, y_new, X, its=500, jitter=1e-5) # increase jitter by one mag \n",
    "preds.append(means)\n",
    "seed = get_seed(dic)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FIFTH ITERATION\n",
    "lib = baseline_greedy(dic, seed, 100)\n",
    "print(lib)\n",
    "lib.sort(key=lambda tup: tup[1])\n",
    "\n",
    "lib_str = [[tup[0] for i, tup in enumerate(lib) if tup[1] == j] for j in range(4)] # generate list of lists of strings\n",
    "lib_str = [''.join(s) for s in itertools.product(*lib_str)] # generate list of strings of 4 aa seqs\n",
    "libs.append(lib_str)\n",
    "encs = np.array([encode_X(x) for x in lib_str]) # generate np array of one-hot encodings\n",
    "\n",
    "index = [X_decode.index(x) for x in lib_str]\n",
    "y_new = np.array([y[i] for i in index])\n",
    "lst_ys.append(y_new)\n",
    "\n",
    "dic, means = get_predictions(encs, y_new, X, its=500, jitter=1e-5) # increase jitter by one mag \n",
    "preds.append(means)\n",
    "seed = get_seed(dic)\n",
    "print(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mean_abs_err(X, y, mu, lib):\n",
    "    \"\"\" Takes in X, true y values, predictions mu, and the sample X's (library) \n",
    "    that the model was trained on, and returns list of abs errors for all y's \n",
    "    not trained on and mean abs error. \n",
    "    \n",
    "    Expects X as one-hot encodings, y and mu as lists of floats, and \n",
    "    lib as list of strings of four aa seqs. \"\"\"\n",
    "    \n",
    "    str_x = [decode_X(x) for x in X]\n",
    "    inds = [i for i, x in enumerate(str_x) if x in lib] # indices of each seq in lib in X\n",
    "    \n",
    "    y_test = list(y) # remove corresponding y's and mu's of seqs in lib\n",
    "    mu_test = mu.copy()\n",
    "    for i in inds:\n",
    "        y_test.pop(i)\n",
    "        mu_test.pop(i)\n",
    "    \n",
    "    errs = [abs(mu - y).item() for mu, y in zip(mu_test, y_test)]\n",
    "    return (y_test, errs), np.mean(np.array(errs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot y vs mean error (for each iteration)\n",
    "\n",
    "errs = [get_mean_abs_err(X, y, mu, lib)[1] for mu, lib in zip(preds, libs)]\n",
    "\n",
    "_ = plt.title(\"Mean absolute error for iterations of Greedy Algorithm\")\n",
    "_ = plt.plot(np.arange(len(errs)), errs, marker='o', linestyle='none')\n",
    "\n",
    "_ = plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Plot mean error for y's vs. y's sorted (for each iteration)\n",
    "\n",
    "abs_errs = [get_mean_abs_err(X, y, mu, lib)[0] for mu, lib in zip(preds, libs)]\n",
    "_ = plt.title(\"Absolute error vs y's tested on for iterations of Greedy Algorithm\")\n",
    "\n",
    "for err in abs_errs: # each err is a tuple of y_test, abs errs\n",
    "    sorted_ind = sorted(range(len(err[0])), key=lambda k: err[0][k]) # get indexes of y sorted\n",
    "    y_sort = np.sort(err[0]) # sort y\n",
    "    err_sort = [err[1][i] for i in sorted_ind]\n",
    "    _ = plt.plot(y_sort, err_sort, marker='.')\n",
    "\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ecdf(data):\n",
    "    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n",
    "    # Number of data points: n\n",
    "    n = len(data)\n",
    "\n",
    "    # x-data for the ECDF: x\n",
    "    x = np.sort(data)\n",
    "\n",
    "    # y-data for the ECDF: y\n",
    "    y = np.arange(1, n + 1) / n\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute ECDF\n",
    "x_val, y_val = ecdf(y)\n",
    "\n",
    "# Generate plot\n",
    "_ = plt.title(\"ECDF of y's and deterministic baselines\")\n",
    "_ = plt.plot(x_val, y_val, marker='.', linestyle='none', label=\"orig y's\")\n",
    "_ = plt.plot(y_seq1, y_val[np.argwhere(x_val == y_seq1)[0][0]], marker='o', label='baseline_fixed')\n",
    "_ = plt.plot(y_seq2, y_val[np.argwhere(x_val == y_seq2)[0][0]], marker='o', label='baseline_vary')\n",
    "_ = plt.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)\n",
    "\n",
    "# Label the axes\n",
    "_ = plt.ylabel('ECDF')\n",
    "_ = plt.xlabel('y')\n",
    "\n",
    "# Display the plot\n",
    "_ = plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot Greedy algorithm baseline with deterministic baselines too\n",
    "\n",
    "d = {'Iterations': [], 'Sampled ys': []}\n",
    "\n",
    "for i in range(len(lst_ys)):\n",
    "    for j in lst_ys[i]:\n",
    "        d['Iterations'].append(i)\n",
    "        d['Sampled ys'].append(j)\n",
    "    \n",
    "df = pd.DataFrame(data=d) # make dataframe of sampled ys to plot on swarmplot\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "_ = plt.title('Baselines: deterministic and Greedy algorithm')\n",
    "ax = sns.swarmplot(x=\"Iterations\", y=\"Sampled ys\", data=df) # swamplot allows for jitter in displaying cluster of ys\n",
    "_ = ax.axhline(y_seq1, color='purple', label='baseline_fixed')\n",
    "_ = ax.axhline(y_seq2, color='black', label='baseline_vary')\n",
    "_ = plt.legend(loc='upper center', bbox_to_anchor=(1.45, 0.8), shadow=True, ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
